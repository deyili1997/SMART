{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import json\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# pandas show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 40\n",
      "8637 8542\n"
     ]
    }
   ],
   "source": [
    "# Load the variables\n",
    "with open('./utils/variables.json', 'r') as file:\n",
    "    variables = json.load(file)\n",
    "\n",
    "SCR_feature_space = variables['SCR_feature_space']\n",
    "LAB_feature_space = variables['LAB_feature_space']\n",
    "train_len = variables['train_len']\n",
    "test_len = variables['test_len']\n",
    "print(len(SCR_feature_space), len(LAB_feature_space))\n",
    "print(train_len, test_len)\n",
    "\n",
    "# get num_processors for parallel computing\n",
    "num_processors = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process UPITT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/52881804/ipykernel_1337165/2764998025.py:2: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  All_onsets = pd.read_csv('/blue/yonghui.wu/lideyi/Personalization_Methodology/NEW_ONSETS.csv')\n"
     ]
    }
   ],
   "source": [
    "#Read in Onsets data and only use KUMC data\n",
    "All_onsets = pd.read_csv('/blue/yonghui.wu/lideyi/Personalization_Methodology/NEW_ONSETS.csv')\n",
    "ext_ONSET = All_onsets.loc[All_onsets.CENTER_NAME == \"UPITT\"].copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read in lab\n",
    "raw_path = '/blue/yonghui.wu/hoyinchan/Data/data2022raw/'\n",
    "data_path = raw_path + \"UPITT\" + '/raw/'\n",
    "ext_LAB = pd.read_csv(data_path + 'AKI_LAB.csv', delimiter = ',', usecols = ['PATID', 'LAB_LOINC', 'SPECIMEN_DATE', 'RESULT_NUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in SCr trajectories\n",
    "SCR_use_cols = ['ONSETS_ENCOUNTERID','PATID','ENCOUNTERID','SPECIMEN_DATE','RESULT_NUM', 'DAYS_SINCE_ADMIT']\n",
    "ext_SCR = pd.read_csv(data_path + \"AKI_LAB_SCR.csv\", delimiter = ',', usecols=SCR_use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#format datatype for merge\n",
    "#exclude those baseline SCr > 3.5, which indicate poor renal functions\n",
    "ext_ONSET = ext_ONSET.loc[ext_ONSET.SERUM_CREAT_BASE < 3.5, :].copy(deep = True)\n",
    "\n",
    "ext_ONSET.loc[:, [\"PATID\", \"ONSETS_ENCOUNTERID\"]] = ext_ONSET[[\"PATID\", \"ONSETS_ENCOUNTERID\"]].astype(str)\n",
    "\n",
    "time_cols = [\"ADMIT_DATE\", \"DISCHARGE_DATE\", \"AKI1_ONSET\", \"AKI2_ONSET\", \"AKI3_ONSET\"]\n",
    "for time_col in time_cols:\n",
    "    ext_ONSET[time_col] = pd.to_datetime(ext_ONSET[time_col], format = \"mixed\")\n",
    "    \n",
    "# binary predictiton task\n",
    "ext_ONSET.loc[:, \"EARLIEST_ONSET_DATE\"] = np.min(ext_ONSET[[\"AKI1_ONSET\", \"AKI2_ONSET\", \"AKI3_ONSET\"]], axis = 1)\n",
    "ext_ONSET.loc[:, \"AKI_LABEL\"] = ext_ONSET[\"EARLIEST_ONSET_DATE\"].notna().astype(int)\n",
    "\n",
    "ext_ONSET.drop([\"CENTER_NAME\", \"SERUM_CREAT_BASE\", \"NONAKI_SINCE_ADMIT\", \"AKI1_ONSET\",\n",
    "           \"AKI2_ONSET\", \"AKI3_ONSET\"], axis = 1, inplace = True)\n",
    "\n",
    "#process data type \n",
    "ext_SCR[\"PATID\"] = ext_SCR[\"PATID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_ONSET_SCR = ext_ONSET.merge(ext_SCR[[\"PATID\", \"SPECIMEN_DATE\", \"RESULT_NUM\"]], on = \"PATID\", how = \"left\")\n",
    "#after merging, process date time\n",
    "ext_ONSET_SCR[\"SPECIMEN_DATE\"] = pd.to_datetime(ext_ONSET_SCR[\"SPECIMEN_DATE\"], format = \"mixed\")\n",
    "#filter out those beyond this hospitalization (we also need history prior to this hospitalization)\n",
    "ext_ONSET_SCR = ext_ONSET_SCR.loc[ext_ONSET_SCR.SPECIMEN_DATE <= ext_ONSET_SCR.DISCHARGE_DATE, :]\n",
    "ext_ONSET_SCR = ext_ONSET_SCR.sort_values(by=['PATID', 'ADMIT_DATE', 'SPECIMEN_DATE'])\n",
    "# get average SCr on the same day\n",
    "ext_ONSET_SCR_avg = ext_ONSET_SCR.groupby(['PATID', 'ONSETS_ENCOUNTERID', 'SPECIMEN_DATE'])['RESULT_NUM'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# append the info back\n",
    "ext_ONSET_SCR_app = ext_ONSET_SCR.loc[:, [\"PATID\", \"ONSETS_ENCOUNTERID\", \"ADMIT_DATE\", \"DISCHARGE_DATE\", \"EARLIEST_ONSET_DATE\", \"AKI_LABEL\"]]\n",
    "ext_ONSET_SCR_app.drop_duplicates(inplace = True)\n",
    "ext_ONSET_SCR_avg = ext_ONSET_SCR_app.merge(ext_ONSET_SCR_avg, on = [\"PATID\", \"ONSETS_ENCOUNTERID\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the prediction point for non-AKI patient\n",
    "ext_non_AKI_pat = ext_ONSET_SCR_avg.loc[ext_ONSET_SCR_avg.AKI_LABEL == 0, [\"PATID\", \"ONSETS_ENCOUNTERID\", \"SPECIMEN_DATE\"]]\n",
    "ext_non_AKI_pat.drop_duplicates(subset = [\"PATID\", \"ONSETS_ENCOUNTERID\"], keep = \"last\", inplace = True)\n",
    "ext_non_AKI_pat.rename(columns = {\"SPECIMEN_DATE\": \"PREDICTION_POINT\"}, inplace = True)\n",
    "ext_ONSET_SCR_avg = ext_ONSET_SCR_avg.merge(ext_non_AKI_pat, on = [\"PATID\", \"ONSETS_ENCOUNTERID\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_ONSET_SCR_avg.loc[ext_ONSET_SCR_avg.AKI_LABEL == 1, \"PREDICTION_POINT\"] = ext_ONSET_SCR_avg.loc[ext_ONSET_SCR_avg.AKI_LABEL == 1, \"EARLIEST_ONSET_DATE\"]\n",
    "#check that we have predicition point for each encounter\n",
    "assert(ext_ONSET_SCR_avg.PREDICTION_POINT.isna().mean() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the time frame we need for SCr is the -8 to -2 days prior to prediction point\n",
    "ext_ONSET_SCR_avg = ext_ONSET_SCR_avg[((ext_ONSET_SCR_avg.SPECIMEN_DATE <= (ext_ONSET_SCR_avg.PREDICTION_POINT) - pd.Timedelta(days=2))) & \\\n",
    "                             (ext_ONSET_SCR_avg.SPECIMEN_DATE >= ext_ONSET_SCR_avg.PREDICTION_POINT - pd.Timedelta(days=8))]\n",
    "#drop patients with less than 2 SCr measurements during the 7-day window\n",
    "# group them and calcualte number of measurements\n",
    "measure_num = ext_ONSET_SCR_avg.groupby('ONSETS_ENCOUNTERID').size()\n",
    "encounterID_to_drop = measure_num[measure_num < 2].index\n",
    "ext_ONSET_SCR_avg = ext_ONSET_SCR_avg.loc[~ext_ONSET_SCR_avg.ONSETS_ENCOUNTERID.isin(encounterID_to_drop), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pivot all the SCr values, that is create features -8 ~ -2 and entries are RESULT_NUM\n",
    "ext_ONSET_SCR_avg[\"DAYS_BEFORE_PREDICTION_POINT\"] = (ext_ONSET_SCR_avg[\"SPECIMEN_DATE\"] - ext_ONSET_SCR_avg[\"PREDICTION_POINT\"]).dt.days\n",
    "\n",
    "#prepare a skleleton to merge on\n",
    "unique_encounterids = list(ext_ONSET_SCR_avg['ONSETS_ENCOUNTERID'].unique())\n",
    "time_window = np.arange(-8, -1)  # from -8 to -2\n",
    "skeleton = pd.MultiIndex.from_product([unique_encounterids, time_window], \n",
    "                                      names=['ONSETS_ENCOUNTERID', 'DAYS_BEFORE_PREDICTION_POINT']).to_frame(index=False)\n",
    "#merge on\n",
    "skeleton = pd.merge(skeleton, ext_ONSET_SCR_avg, on=['ONSETS_ENCOUNTERID', 'DAYS_BEFORE_PREDICTION_POINT'], how='left')\n",
    "\n",
    "#pivot\n",
    "ONSET_SCR_formatted = skeleton.pivot(index='ONSETS_ENCOUNTERID', \n",
    "                                          columns='DAYS_BEFORE_PREDICTION_POINT', \n",
    "                                          values='RESULT_NUM').reset_index()\n",
    "\n",
    "# get other info back\n",
    "ONSET_SCR_app2 = ext_ONSET_SCR_avg.loc[:, [\"PATID\", \"ONSETS_ENCOUNTERID\", \"ADMIT_DATE\", \"DISCHARGE_DATE\", \n",
    "                                       \"PREDICTION_POINT\", \"AKI_LABEL\"]]\n",
    "ONSET_SCR_app2.drop_duplicates(inplace = True)\n",
    "ext_ONSET_SCR_formatted = ONSET_SCR_formatted.merge(ONSET_SCR_app2, on = \"ONSETS_ENCOUNTERID\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only keep the earliest encounter of each patient\n",
    "ext_ONSET_SCR_formatted = ext_ONSET_SCR_formatted.sort_values(by=['PATID', 'ADMIT_DATE'])\n",
    "ext_ONSET_SCR_formatted = ext_ONSET_SCR_formatted.drop_duplicates(subset='PATID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_LAB[\"PATID\"] = ext_LAB[\"PATID\"].astype(str)\n",
    "# merge the lab \n",
    "ext_ONSET_SCR_LAB = ext_ONSET_SCR_formatted.merge(ext_LAB, on = \"PATID\", how = \"left\")\n",
    "ext_ONSET_SCR_LAB[\"SPECIMEN_DATE\"] = pd.to_datetime(ext_ONSET_SCR_LAB[\"SPECIMEN_DATE\"], format = \"mixed\")\n",
    "ext_ONSET_SCR_LAB = ext_ONSET_SCR_LAB[(ext_ONSET_SCR_LAB.SPECIMEN_DATE <= (ext_ONSET_SCR_LAB.PREDICTION_POINT - pd.Timedelta(days=2))) & \\\n",
    "                              (ext_ONSET_SCR_LAB.SPECIMEN_DATE >= ext_ONSET_SCR_LAB.ADMIT_DATE - pd.Timedelta(days=8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we only keep the lastest result of a certain lab within the time window\n",
    "ext_ONSET_SCR_LAB_temp = ext_ONSET_SCR_LAB.sort_values(by=['PATID', 'ONSETS_ENCOUNTERID', 'LAB_LOINC', 'SPECIMEN_DATE'])\n",
    "ext_ONSET_SCR_LAB_temp = ext_ONSET_SCR_LAB_temp.groupby(['PATID', 'ONSETS_ENCOUNTERID', 'LAB_LOINC']).last().reset_index()\n",
    "#turn lab into feature columns\n",
    "ext_LAB_info = ext_ONSET_SCR_LAB_temp.pivot(index='ONSETS_ENCOUNTERID', columns='LAB_LOINC', values='RESULT_NUM')\n",
    "ext_LAB_info = ext_LAB_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# align feature space with the internal dataset\n",
    "ext_LAB_info = ext_LAB_info[[\"ONSETS_ENCOUNTERID\"] + LAB_feature_space]\n",
    "#merge them back to the original dataframe\n",
    "ext_ONSET_SCR_LAB = ext_ONSET_SCR_formatted.merge(ext_LAB_info, on = 'ONSETS_ENCOUNTERID', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the External Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r start_date\n",
    "%store -r split_date\n",
    "%store -r end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter patients that from the same time range and sample number\n",
    "ext_test = ext_ONSET_SCR_LAB[(ext_ONSET_SCR_LAB.ADMIT_DATE >= split_date) & (ext_ONSET_SCR_LAB.ADMIT_DATE < end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ext_test_sampled, _ = train_test_split(\n",
    "    ext_test, \n",
    "    test_size=(len(ext_test) - test_len) / len(ext_test), \n",
    "    random_state=88, \n",
    "    stratify=ext_test['AKI_LABEL']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_test_sampled.to_csv('/blue/yonghui.wu/lideyi/Personalization_Methodology/ext_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_test_sampled = pd.read_csv('/blue/yonghui.wu/lideyi/Personalization_Methodology/ext_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-index the external dataset such that the index start from train_len\n",
    "ext_test_sampled.index = range(train_len, train_len + len(ext_test_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Internal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_dataset = pd.read_csv(\"/blue/yonghui.wu/lideyi/Personalization_Methodology/dataset.csv\")\n",
    "assert(len(int_dataset) == (train_len + test_len))\n",
    "int_train = int_dataset.iloc[:train_len, :].copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# align the feature space with the internal dataset\n",
    "int_train.columns = int_train.columns.map(str)\n",
    "ext_test_sampled.columns = ext_test_sampled.columns.map(str)\n",
    "ext_test_sampled = ext_test_sampled.loc[:, SCR_feature_space + LAB_feature_space + ['AKI_LABEL']]\n",
    "# concatenate the two datasets\n",
    "dataset_full = pd.concat([int_train, ext_test_sampled], axis = 0)\n",
    "dataset_full.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-8</th>\n",
       "      <th>-7</th>\n",
       "      <th>-6</th>\n",
       "      <th>-5</th>\n",
       "      <th>-4</th>\n",
       "      <th>-3</th>\n",
       "      <th>-2</th>\n",
       "      <th>14979-9</th>\n",
       "      <th>1742-6</th>\n",
       "      <th>17861-6</th>\n",
       "      <th>19123-9</th>\n",
       "      <th>1920-8</th>\n",
       "      <th>1963-8</th>\n",
       "      <th>1975-2</th>\n",
       "      <th>2075-0</th>\n",
       "      <th>2345-7</th>\n",
       "      <th>26464-8</th>\n",
       "      <th>2777-1</th>\n",
       "      <th>2823-3</th>\n",
       "      <th>2885-2</th>\n",
       "      <th>2951-2</th>\n",
       "      <th>3094-0</th>\n",
       "      <th>32623-1</th>\n",
       "      <th>33037-3</th>\n",
       "      <th>4544-3</th>\n",
       "      <th>50560-2</th>\n",
       "      <th>53326-5</th>\n",
       "      <th>5905-5</th>\n",
       "      <th>61151-7</th>\n",
       "      <th>6301-6</th>\n",
       "      <th>6768-6</th>\n",
       "      <th>704-7</th>\n",
       "      <th>706-2</th>\n",
       "      <th>711-2</th>\n",
       "      <th>713-8</th>\n",
       "      <th>718-7</th>\n",
       "      <th>731-0</th>\n",
       "      <th>736-9</th>\n",
       "      <th>742-7</th>\n",
       "      <th>751-8</th>\n",
       "      <th>770-8</th>\n",
       "      <th>777-3</th>\n",
       "      <th>785-6</th>\n",
       "      <th>786-4</th>\n",
       "      <th>787-2</th>\n",
       "      <th>788-0</th>\n",
       "      <th>789-8</th>\n",
       "      <th>AKI_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>51.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>33.4</td>\n",
       "      <td>96.6</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>138.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>33.3</td>\n",
       "      <td>95.6</td>\n",
       "      <td>14.6</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.03</td>\n",
       "      <td>0.990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>132.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.021</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>29.1</td>\n",
       "      <td>31.4</td>\n",
       "      <td>92.8</td>\n",
       "      <td>15.7</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.62</td>\n",
       "      <td>32.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>134.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>33.8</td>\n",
       "      <td>87.1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>124.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.018</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>21.8</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>97.8</td>\n",
       "      <td>16.8</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     -8     -7     -6    -5    -4    -3    -2  14979-9  1742-6  17861-6  \\\n",
       "0   NaN    NaN    NaN   NaN  0.72  0.95  0.88      NaN     NaN      9.3   \n",
       "1   NaN  1.210    NaN  0.97  1.04  0.93  0.96      NaN    22.0      8.9   \n",
       "2  1.03  0.990    NaN   NaN  1.01  0.91  0.82      NaN    22.0      8.8   \n",
       "3   NaN    NaN    NaN  0.69  0.90  0.73  0.62     32.2    28.0      7.8   \n",
       "4  0.60  0.615  0.685  0.62  0.50  0.53  0.56      NaN    12.0      7.8   \n",
       "\n",
       "   19123-9  1920-8  1963-8  1975-2  2075-0  2345-7  26464-8  2777-1  2823-3  \\\n",
       "0      1.9     NaN    25.0     NaN   105.0    94.0     10.9     4.0     4.1   \n",
       "1      1.7    19.0    30.0     0.4   104.0    92.0     13.6     2.4     3.9   \n",
       "2      2.4    25.0    24.0     0.6   101.0   119.0     17.1     3.7     4.1   \n",
       "3      2.8    24.0    24.0     1.3   104.0   114.0     15.7     NaN     4.2   \n",
       "4      1.7    21.0    26.0     0.5    94.0   131.0     59.0     2.9     4.6   \n",
       "\n",
       "   2885-2  2951-2  3094-0  32623-1  33037-3  4544-3  50560-2  53326-5  5905-5  \\\n",
       "0     NaN   137.0    13.0      7.0      7.0    39.0      NaN      NaN     9.0   \n",
       "1     5.9   138.0    21.0     10.3      4.0    40.2      NaN      NaN     NaN   \n",
       "2     6.4   132.0    19.0      9.9      7.0    35.5      6.0    1.021     8.0   \n",
       "3     6.5   134.0    11.0      8.5      6.0    32.5      7.0    1.050     NaN   \n",
       "4     4.4   124.0    15.0      7.8      4.0    22.7      5.0    1.018     2.0   \n",
       "\n",
       "   61151-7  6301-6  6768-6  704-7  706-2  711-2  713-8  718-7  731-0  736-9  \\\n",
       "0      NaN     NaN     NaN    0.0    0.0    0.4    4.0   13.0    3.9   36.0   \n",
       "1      3.5     1.0    37.0    NaN    NaN    NaN    NaN   13.4    NaN    NaN   \n",
       "2      2.6     NaN   115.0    0.1    1.0    0.1    0.0   11.1    1.5    9.0   \n",
       "3      4.0     1.0    53.0    NaN    NaN    NaN    NaN   11.0    NaN    NaN   \n",
       "4      2.3     1.3    85.0    0.0    0.0    0.0    0.0    7.4   21.8   84.0   \n",
       "\n",
       "   742-7  751-8  770-8  777-3  785-6  786-4  787-2  788-0  789-8  AKI_LABEL  \n",
       "0    1.0    5.6   51.0  228.0   32.2   33.4   96.6   13.5   4.04          0  \n",
       "1    NaN    NaN    NaN  168.0   31.8   33.3   95.6   14.6   4.20          0  \n",
       "2    1.3   14.2   82.0  165.0   29.1   31.4   92.8   15.7   3.82          0  \n",
       "3    NaN    NaN    NaN  152.0   29.5   33.8   87.1   14.0   3.73          0  \n",
       "4    0.4    1.8    7.0   66.0   32.0   32.7   97.8   16.8   2.32          1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Data Overlap Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_full = dataset_full.loc[:, SCR_feature_space]\n",
    "LAB_full = dataset_full.loc[:, LAB_feature_space]\n",
    "SCR_full_bin = SCR_full.notna().astype(int)\n",
    "LAB_full_bin = LAB_full.notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Data_Overlap_Rates_Computing import parallel_overlap_matrix_comp, check_matrix_sanity, calculate_overlap_rate_SCR, calculate_overlap_rate_LAB\n",
    "%store -r normal_distribution_SCR\n",
    "%store -r lab_overlap_weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCR_overlap = parallel_overlap_matrix_comp(SCR_full_bin, num_processors, calculate_overlap_rate_SCR, normal_distribution_SCR)\n",
    "# check_matrix_sanity(SCR_overlap)\n",
    "# print(np.median(SCR_overlap))\n",
    "# print(np.mean(SCR_overlap))\n",
    "# np.save('/blue/yonghui.wu/lideyi/Personalization_Methodology/SCR_overlap_external.npy', SCR_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LAB_overlap = parallel_overlap_matrix_comp(LAB_full_bin, num_processors, calculate_overlap_rate_LAB, lab_overlap_weighting)\n",
    "# check_matrix_sanity(LAB_overlap)\n",
    "# print(np.median(LAB_overlap))\n",
    "# print(np.mean(LAB_overlap))\n",
    "# np.save('/blue/yonghui.wu/lideyi/Personalization_Methodology/lab_overlap_external.npy', LAB_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Distance_Computing import parallel_distance_matrix, get_DTW_distance\n",
    "from utils.Z_Helping_Functions import translate_dist_mtx_to_simi, fast_argsort, min_max_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for SCR we do not apply the same normalization as LAB, since the values are already in the same range (unit)\n",
    "# SCR_DTW_dist_full = parallel_distance_matrix(SCR_full, num_processors, get_DTW_distance)\n",
    "# np.save('/blue/yonghui.wu/lideyi/Personalization_Methodology/SCR_DTW_dist_external.npy', SCR_DTW_dist_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_DTW_dist_full = np.load('/blue/yonghui.wu/lideyi/Personalization_Methodology/SCR_DTW_dist_external.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfrom distance mtx to similarity score mtx by min-max normalization and substration from 1\n",
    "SCR_DTW_simi_full = translate_dist_mtx_to_simi(SCR_DTW_dist_full)\n",
    "# sort similarity score mtx into idx matrix by most similar rank highest. This is for the entire dataset, train + test\n",
    "SCR_DTW_idx_full = fast_argsort(SCR_DTW_simi_full, num_processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute Missing Values for SCR and LAB Respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCR, since it just row wise we do not need to split into train and test\n",
    "SCR_full = SCR_full.interpolate(method='linear', axis = 1)\n",
    "SCR_full = SCR_full.bfill(axis=1)\n",
    "SCR_full = SCR_full.ffill(axis=1)\n",
    "# after imputation, we can safely split into train and test\n",
    "SCR_train = SCR_full.iloc[:train_len, :].copy(deep = True)\n",
    "SCR_test = SCR_full.iloc[train_len:, :].copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# it is of note that the imputation is done on the lab train and lab test separately\n",
    "# this is to avoid data leakage\n",
    "LAB_train = int_train.loc[:, LAB_feature_space].copy(deep = True)\n",
    "LAB_test = ext_test_sampled.loc[:, LAB_feature_space].copy(deep = True)\n",
    "# lab normalization\n",
    "LAB_train = (LAB_train - LAB_train.min(skipna=True)) / (LAB_train.max(skipna=True) - LAB_train.min(skipna=True))\n",
    "LAB_test = (LAB_test - LAB_test.min(skipna=True)) / (LAB_test.max(skipna=True) - LAB_test.min(skipna=True))\n",
    "# impute lab missing values\n",
    "imputer = IterativeImputer(missing_values=np.nan, max_iter=1000, random_state=42)\n",
    "imputer.fit(LAB_train)\n",
    "LAB_train_temp = imputer.transform(LAB_train)\n",
    "LAB_test_temp = imputer.transform(LAB_test)\n",
    "LAB_train.loc[:, :] = LAB_train_temp\n",
    "LAB_test.loc[:, :] = LAB_test_temp\n",
    "# Concate train and test\n",
    "LAB_full = pd.concat([LAB_train, LAB_test], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean, Cosine and Manhattan Matrix of SCR and LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Distance_Computing import compute_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCR\n",
    "# Euclidean\n",
    "SCR_Euc_simi_full, SCR_Euc_idx_full, _, _ = compute_similarity(SCR_full, 'euclidean', train_len, num_processors)\n",
    "# Cosine\n",
    "SCR_Cos_simi_full, SCR_Cos_idx_full, _, _ = compute_similarity(SCR_full, 'cosine', train_len, num_processors)\n",
    "# Manhattan\n",
    "SCR_Manh_simi_full, SCR_Manh_idx_full, _, _ = compute_similarity(SCR_full, 'manhattan', train_len, num_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LAB\n",
    "# Euclidean\n",
    "LAB_Euc_simi_full, LAB_Euc_idx_full, _, _ = compute_similarity(LAB_full, 'euclidean', train_len, num_processors)\n",
    "# Cosine\n",
    "LAB_Cos_simi_full, LAB_Cos_idx_full, _, _ = compute_similarity(LAB_full, 'cosine', train_len, num_processors)\n",
    "# Manhattan\n",
    "LAB_Manh_simi_full, LAB_Manh_idx_full, _, _ = compute_similarity(LAB_full, 'manhattan', train_len, num_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simi here is the un-ordered, normalized similarity score matrix, idx is the ordered paitent index matrix\n",
    "# nw stands for not weighted by data overlap rates matrix\n",
    "nw_fea_arrs_dict = {\"SCR\": {\"DTW\": {\"simi\": {\"full\": SCR_DTW_simi_full}, \"idx\": {\"full\": SCR_DTW_idx_full}}, \n",
    "                            \"Euc\": {\"simi\": {\"full\": SCR_Euc_simi_full}, \"idx\": {\"full\": SCR_Euc_idx_full}}, \n",
    "                            \"Cos\": {\"simi\": {\"full\": SCR_Cos_simi_full}, \"idx\": {\"full\": SCR_Cos_idx_full}}, \n",
    "                            \"Manh\":{\"simi\": {\"full\": SCR_Manh_simi_full}, \"idx\": {\"full\": SCR_Manh_idx_full}}}, \n",
    "                    \"LAB\": {\"Euc\": {\"simi\": {\"full\": LAB_Euc_simi_full}, \"idx\": {\"full\": LAB_Euc_idx_full}}, \n",
    "                            \"Cos\": {\"simi\": {\"full\": LAB_Cos_simi_full}, \"idx\": {\"full\": LAB_Cos_idx_full}}, \n",
    "                            \"Manh\":{\"simi\": {\"full\": LAB_Manh_simi_full}, \"idx\": {\"full\": LAB_Manh_idx_full}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overlap Rates Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Distance_Computing import overlap_rates_weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read pre-computed pairwise data overlap rates\n",
    "SCR_overlap_full = np.load('/blue/yonghui.wu/lideyi/Personalization_Methodology/SCR_overlap_external.npy')\n",
    "LAB_overlap_full = np.load('/blue/yonghui.wu/lideyi/Personalization_Methodology/lab_overlap_external.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# on full data (for testing)\n",
    "# SCR\n",
    "SCR_DTW_simi_wt_full, SCR_DTW_idx_wt_full = overlap_rates_weighting(SCR_overlap_full, nw_fea_arrs_dict[\"SCR\"][\"DTW\"][\"simi\"][\"full\"], num_processors)\n",
    "SCR_Euc_simi_wt_full, SCR_Euc_idx_wt_full = overlap_rates_weighting(SCR_overlap_full, nw_fea_arrs_dict[\"SCR\"][\"Euc\"][\"simi\"][\"full\"], num_processors)\n",
    "SCR_Cos_simi_wt_full, SCR_Cos_idx_wt_full = overlap_rates_weighting(SCR_overlap_full, nw_fea_arrs_dict[\"SCR\"][\"Cos\"][\"simi\"][\"full\"], num_processors)\n",
    "SCR_Manh_simi_wt_full, SCR_Manh_idx_wt_full = overlap_rates_weighting(SCR_overlap_full, nw_fea_arrs_dict[\"SCR\"][\"Manh\"][\"simi\"][\"full\"], num_processors)\n",
    "\n",
    "# LAB\n",
    "LAB_Euc_simi_wt_full, LAB_Euc_idx_wt_full = overlap_rates_weighting(LAB_overlap_full, nw_fea_arrs_dict[\"LAB\"][\"Euc\"][\"simi\"][\"full\"], num_processors)\n",
    "LAB_Cos_simi_wt_full, LAB_Cos_idx_wt_full = overlap_rates_weighting(LAB_overlap_full, nw_fea_arrs_dict[\"LAB\"][\"Cos\"][\"simi\"][\"full\"], num_processors)\n",
    "LAB_Manh_simi_wt_full, LAB_Manh_idx_wt_full = overlap_rates_weighting(LAB_overlap_full, nw_fea_arrs_dict[\"LAB\"][\"Manh\"][\"simi\"][\"full\"], num_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simi here is the un-ordered, normalized similarity score matrix, idx is the ordered paitent index matrix\n",
    "# nw stands for not weighted by data overlap rates matrix\n",
    "wt_fea_arrs_dict = {\"SCR\": {\"DTW\": {\"simi\": {\"full\": SCR_DTW_simi_wt_full}, \"idx\": {\"full\": SCR_DTW_idx_wt_full}}, \n",
    "                            \"Euc\": {\"simi\": {\"full\": SCR_Euc_simi_wt_full}, \"idx\": {\"full\": SCR_Euc_idx_wt_full}}, \n",
    "                            \"Cos\": {\"simi\": {\"full\": SCR_Cos_simi_wt_full}, \"idx\": {\"full\": SCR_Cos_idx_wt_full}}, \n",
    "                            \"Manh\":{\"simi\": {\"full\": SCR_Manh_simi_wt_full}, \"idx\": {\"full\": SCR_Manh_idx_wt_full}}}, \n",
    "                    \"LAB\": {\"Euc\": {\"simi\": {\"full\": LAB_Euc_simi_wt_full}, \"idx\": {\"full\": LAB_Euc_idx_wt_full}}, \n",
    "                            \"Cos\": {\"simi\": {\"full\": LAB_Cos_simi_wt_full}, \"idx\": {\"full\": LAB_Cos_idx_wt_full}}, \n",
    "                            \"Manh\":{\"simi\": {\"full\": LAB_Manh_simi_wt_full}, \"idx\": {\"full\": LAB_Manh_idx_wt_full}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Best Distance Measures on Test Set Using KNN/LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_idx = dataset_full.iloc[:train_len, :].index\n",
    "test_idx = dataset_full.iloc[train_len:, :].index\n",
    "y_test = np.array(ext_test_sampled['AKI_LABEL'])\n",
    "y_full = np.array(dataset_full['AKI_LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Testing import process_idx_arr_for_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.04s/it]\n",
      "100%|██████████| 4/4 [00:16<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# SCR\n",
    "SCR_idx_y_nw_dict_test = {}\n",
    "SCR_idx_y_wt_dict_test = {}\n",
    "for dist_measure, arrs in tqdm(nw_fea_arrs_dict[\"SCR\"].items()):\n",
    "    idx_arr_test_clean, y_test_arr = process_idx_arr_for_test(train_idx, test_idx, arrs[\"idx\"][\"full\"], y_full)\n",
    "    SCR_idx_y_nw_dict_test[dist_measure] = {\"idx\": idx_arr_test_clean, \"label\": y_test_arr}\n",
    "for dist_measure, arrs in tqdm(wt_fea_arrs_dict[\"SCR\"].items()):\n",
    "    idx_arr_test_clean, y_test_arr = process_idx_arr_for_test(train_idx, test_idx, arrs[\"idx\"][\"full\"], y_full)\n",
    "    SCR_idx_y_wt_dict_test[dist_measure] = {\"idx\": idx_arr_test_clean, \"label\": y_test_arr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.74s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# LAB\n",
    "LAB_idx_y_nw_dict_test = {}\n",
    "LAB_idx_y_wt_dict_test = {}\n",
    "for dist_measure, arrs in tqdm(nw_fea_arrs_dict[\"LAB\"].items()):\n",
    "    idx_arr_test_clean, y_test_arr = process_idx_arr_for_test(train_idx, test_idx, arrs[\"idx\"][\"full\"], y_full)\n",
    "    LAB_idx_y_nw_dict_test[dist_measure] = {\"idx\": idx_arr_test_clean, \"label\": y_test_arr}\n",
    "for dist_measure, arrs in tqdm(wt_fea_arrs_dict[\"LAB\"].items()):\n",
    "    idx_arr_test_clean, y_test_arr = process_idx_arr_for_test(train_idx, test_idx, arrs[\"idx\"][\"full\"], y_full)\n",
    "    LAB_idx_y_wt_dict_test[dist_measure] = {\"idx\": idx_arr_test_clean, \"label\": y_test_arr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# here we reduce the number of k to be tested\n",
    "k_sizes_test = [i for i in range(10, 201, 10)]\n",
    "print(len(k_sizes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Testing import evluate_on_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 14.94it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 14.95it/s]\n"
     ]
    }
   ],
   "source": [
    "SCR_DTW_control_KNN, _ = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"DTW\", \"Euc\", \n",
    "                                             y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                             LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [11:41<00:00, 35.07s/it]\n",
      " 35%|███▌      | 7/20 [03:31<07:13, 33.35s/it]"
     ]
    }
   ],
   "source": [
    "SCR_DTW_control_LR, _ = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"DTW\", \"Euc\", \n",
    "                                             y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                             LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Euc_control_KNN, LAB_Euc_control_KNN = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Euc\", \"Euc\", y_test, k_sizes_test, \n",
    "                                                               SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                               LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Euc_control_LR, LAB_Euc_control_LR = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Euc\", \"Euc\", \n",
    "                                                             y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                             LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Cos_control_KNN, LAB_Cos_control_KNN = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Cos\", \"Cos\", \n",
    "                                                               y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                               LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Cos_control_LR, LAB_Cos_control_LR = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Cos\", \"Cos\", \n",
    "                                                               y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                               LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Manh_control_KNN, LAB_Manh_control_KNN = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Manh\", \"Manh\", \n",
    "                                                                 y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                                 LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCR_Manh_control_LR, LAB_Manh_control_LR = evluate_on_test_set(SCR_train, SCR_test, LAB_train, LAB_test, \"Manh\", \"Manh\", \n",
    "                                                                y_test, k_sizes_test, SCR_idx_y_nw_dict_test, SCR_idx_y_wt_dict_test, \n",
    "                                                                LAB_idx_y_nw_dict_test, LAB_idx_y_wt_dict_test, \"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and Prove: Data Overlap Rates Weighting can Improve Performance in External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Plotting import plot_metric_along_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10)) \n",
    "\n",
    "metric = \"AUPRC\"\n",
    "\n",
    "plot_metric_along_k(axs[0,0], k_sizes_test, SCR_DTW_control_KNN[\"NW\"][metric], SCR_DTW_control_KNN[\"WT\"][metric], \"SCR-voting: DTW\", metric)\n",
    "plot_metric_along_k(axs[0,1], k_sizes_test, SCR_Euc_control_KNN[\"NW\"][metric], SCR_Euc_control_KNN[\"WT\"][metric], \"SCR-voting: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1,0], k_sizes_test, SCR_Cos_control_KNN[\"NW\"][metric], SCR_Cos_control_KNN[\"WT\"][metric], \"SCR-voting: Cosine\", metric)\n",
    "plot_metric_along_k(axs[1,1], k_sizes_test, SCR_Manh_control_KNN[\"NW\"][metric], SCR_Manh_control_KNN[\"WT\"][metric], \"SCR-voting: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10)) \n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "plot_metric_along_k(axs[0,0], k_sizes_test, SCR_DTW_control_KNN[\"NW\"][metric], SCR_DTW_control_KNN[\"WT\"][metric], \"SCR-voting: DTW\", metric)\n",
    "plot_metric_along_k(axs[0,1], k_sizes_test, SCR_Euc_control_KNN[\"NW\"][metric], SCR_Euc_control_KNN[\"WT\"][metric], \"SCR-voting: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1,0], k_sizes_test, SCR_Cos_control_KNN[\"NW\"][metric], SCR_Cos_control_KNN[\"WT\"][metric], \"SCR-voting: Cosine\", metric)\n",
    "plot_metric_along_k(axs[1,1], k_sizes_test, SCR_Manh_control_KNN[\"NW\"][metric], SCR_Manh_control_KNN[\"WT\"][metric], \"SCR-voting: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10)) \n",
    "\n",
    "metric = \"AUPRC\"\n",
    "\n",
    "plot_metric_along_k(axs[0,0], k_sizes_test, SCR_DTW_control_LR[\"NW\"][metric], SCR_DTW_control_LR[\"WT\"][metric], \"SCR-LR: DTW\", metric)\n",
    "plot_metric_along_k(axs[0,1], k_sizes_test, SCR_Euc_control_LR[\"NW\"][metric], SCR_Euc_control_LR[\"WT\"][metric], \"SCR-LR: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1,0], k_sizes_test, SCR_Cos_control_LR[\"NW\"][metric], SCR_Cos_control_LR[\"WT\"][metric], \"SCR-LR: Cosine\", metric)\n",
    "plot_metric_along_k(axs[1,1], k_sizes_test, SCR_Manh_control_LR[\"NW\"][metric], SCR_Manh_control_LR[\"WT\"][metric], \"SCR-LR: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10)) \n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "plot_metric_along_k(axs[0,0], k_sizes_test, SCR_DTW_control_LR[\"NW\"][metric], SCR_DTW_control_LR[\"WT\"][metric], \"SCR-LR: DTW\", metric)\n",
    "plot_metric_along_k(axs[0,1], k_sizes_test, SCR_Euc_control_LR[\"NW\"][metric], SCR_Euc_control_LR[\"WT\"][metric], \"SCR-LR: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1,0], k_sizes_test, SCR_Cos_control_LR[\"NW\"][metric], SCR_Cos_control_LR[\"WT\"][metric], \"SCR-LR: Cosine\", metric)\n",
    "plot_metric_along_k(axs[1,1], k_sizes_test, SCR_Manh_control_LR[\"NW\"][metric], SCR_Manh_control_LR[\"WT\"][metric], \"SCR-LR: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4)) \n",
    "\n",
    "metric = \"AUPRC\"\n",
    "\n",
    "plot_metric_along_k(axs[0], k_sizes_test, LAB_Euc_control_KNN[\"NW\"][metric], LAB_Euc_control_KNN[\"WT\"][metric], \"LAB-voting: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1], k_sizes_test, LAB_Cos_control_KNN[\"NW\"][metric], LAB_Cos_control_KNN[\"WT\"][metric], \"LAB-voting: Cosine\", metric)\n",
    "plot_metric_along_k(axs[2], k_sizes_test, LAB_Manh_control_KNN[\"NW\"][metric], LAB_Manh_control_KNN[\"WT\"][metric], \"LAB-voting: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4)) \n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "plot_metric_along_k(axs[0], k_sizes_test, LAB_Euc_control_KNN[\"NW\"][metric], LAB_Euc_control_KNN[\"WT\"][metric], \"LAB-voting: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1], k_sizes_test, LAB_Cos_control_KNN[\"NW\"][metric], LAB_Cos_control_KNN[\"WT\"][metric], \"LAB-voting: Cosine\", metric)\n",
    "plot_metric_along_k(axs[2], k_sizes_test, LAB_Manh_control_KNN[\"NW\"][metric], LAB_Manh_control_KNN[\"WT\"][metric], \"LAB-voting: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4)) \n",
    "\n",
    "metric = \"AUPRC\"\n",
    "\n",
    "plot_metric_along_k(axs[0], k_sizes_test, LAB_Euc_control_LR[\"NW\"][metric], LAB_Euc_control_LR[\"WT\"][metric], \"LAB-LR: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1], k_sizes_test, LAB_Cos_control_LR[\"NW\"][metric], LAB_Cos_control_LR[\"WT\"][metric], \"LAB-LR: Cosine\", metric)\n",
    "plot_metric_along_k(axs[2], k_sizes_test, LAB_Manh_control_LR[\"NW\"][metric], LAB_Manh_control_LR[\"WT\"][metric], \"LAB-LR: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4)) \n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "plot_metric_along_k(axs[0], k_sizes_test, LAB_Euc_control_LR[\"NW\"][metric], LAB_Euc_control_LR[\"WT\"][metric], \"LAB-LR: Euclidean\", metric)\n",
    "plot_metric_along_k(axs[1], k_sizes_test, LAB_Cos_control_LR[\"NW\"][metric], LAB_Cos_control_LR[\"WT\"][metric], \"LAB-LR: Cosine\", metric)\n",
    "plot_metric_along_k(axs[2], k_sizes_test, LAB_Manh_control_LR[\"NW\"][metric], LAB_Manh_control_LR[\"WT\"][metric], \"LAB-LR: Manhattan\", metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Final Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Testing import test_final_personalized_model, get_best_weights, combine_best_weights_for_test, KNN, predict_by_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search_table = pd.read_csv(\"/blue/yonghui.wu/lideyi/Personalization_Methodology/grid_search_table_imput2_LR.csv\", index_col = 0)\n",
    "best_distance_measures = dict()\n",
    "for column in grid_search_table.columns:\n",
    "    mode_value = grid_search_table[column].mode()[0]\n",
    "    best_distance_measures[column] = mode_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt_SCR_simi_nw_full = nw_fea_arrs_dict[\"SCR\"][best_distance_measures[\"SCR NW\"]][\"simi\"][\"full\"]\n",
    "opt_LAB_simi_nw_full = nw_fea_arrs_dict[\"LAB\"][best_distance_measures[\"LAB NW\"]][\"simi\"][\"full\"]\n",
    "opt_SCR_simi_wt_full = wt_fea_arrs_dict[\"SCR\"][best_distance_measures[\"SCR WT\"]][\"simi\"][\"full\"]\n",
    "opt_LAB_simi_wt_full = wt_fea_arrs_dict[\"LAB\"][best_distance_measures[\"LAB WT\"]][\"simi\"][\"full\"]\n",
    "\n",
    "opt_measure_simi_nw_full_dict = {\"SCR\": opt_SCR_simi_nw_full, \"LAB\": opt_LAB_simi_nw_full}\n",
    "opt_measure_simi_wt_full_dict = {\"SCR\": opt_SCR_simi_wt_full, \"LAB\": opt_LAB_simi_wt_full}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these are the no nan features\n",
    "X_train = pd.concat([SCR_train, LAB_train], axis = 1)\n",
    "X_test = pd.concat([SCR_test, LAB_test], axis = 1)\n",
    "assert X_train.shape[1] == X_test.shape[1]\n",
    "# assert no nan values\n",
    "assert not X_train.isnull().values.any(), \"The DataFrame contains NaN values!\"\n",
    "assert not X_test.isnull().values.any(), \"The DataFrame contains NaN values!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 1: Optimized distance measure + optimized feature type weights + no overlap rates weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model_performance_nw_KNN = test_final_personalized_model(X_train, X_test, k_sizes_test, grid_search_table, \n",
    "                                                           train_idx, test_idx, y_full, y_test, opt_measure_simi_nw_full_dict, \n",
    "                                                           num_processors, \"KNN\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model_performance_nw_LR = test_final_personalized_model(X_train, X_test, k_sizes_test, grid_search_table, \n",
    "                                                           train_idx, test_idx, y_full, y_test, opt_measure_simi_nw_full_dict, \n",
    "                                                           num_processors, \"LR\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 2: Optimized distance measure + optimized feature type weights + overlap rates weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model_performance_wt_KNN = test_final_personalized_model(X_train, X_test, k_sizes_test, grid_search_table, \n",
    "                                                           train_idx, test_idx, y_full, y_test, opt_measure_simi_wt_full_dict, \n",
    "                                                           num_processors, \"KNN\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_model_performance_wt_LR = test_final_personalized_model(X_train, X_test, k_sizes_test, grid_search_table, \n",
    "                                                           train_idx, test_idx, y_full, y_test, opt_measure_simi_wt_full_dict, \n",
    "                                                           num_processors, \"LR\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 3: Fixed Euclidean distance + fixed feature type weights, k = 20 + no overlap rates weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_k = 20\n",
    "base_fix_distance_performance_nw_KNN = {\"AUPRC\": [], \"AUROC\": []}\n",
    "base_fix_distance_performance_nw_LR = {\"AUPRC\": [], \"AUROC\": []}\n",
    "A_nw, B_nw = eval(get_best_weights(grid_search_table, base_k, False))\n",
    "base_fix_SCR_simi_nw_full = nw_fea_arrs_dict[\"SCR\"][\"DTW\"][\"simi\"][\"full\"]\n",
    "base_fix_LAB_simi_nw_full = nw_fea_arrs_dict[\"LAB\"][\"Euc\"][\"simi\"][\"full\"]\n",
    "fix_combined_weights_dict_nw = combine_best_weights_for_test(base_fix_SCR_simi_nw_full, base_fix_LAB_simi_nw_full, A_nw, B_nw, train_idx, test_idx, y_full, num_processors)\n",
    "\n",
    "for k in tqdm(k_sizes_test):\n",
    "    base_AUPRC_nw_KNN, base_AUROC_nw_KNN = KNN(fix_combined_weights_dict_nw, k, y_test)\n",
    "    base_fix_distance_performance_nw_KNN[\"AUPRC\"].append(base_AUPRC_nw_KNN)\n",
    "    base_fix_distance_performance_nw_KNN[\"AUROC\"].append(base_AUROC_nw_KNN)\n",
    "    base_AUPRC_nw_LR, base_AUROC_nw_LR = predict_by_LR(X_train, X_test, fix_combined_weights_dict_nw, k, y_test)\n",
    "    base_fix_distance_performance_nw_LR[\"AUPRC\"].append(base_AUPRC_nw_LR)\n",
    "    base_fix_distance_performance_nw_LR[\"AUROC\"].append(base_AUROC_nw_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 4: Optimized distance measure + fixed feature type weights, k = 20 + no overlap rates weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_opt_distance_performance_nw_KNN = {\"AUPRC\": [], \"AUROC\": []}\n",
    "base_opt_distance_performance_nw_LR = {\"AUPRC\": [], \"AUROC\": []}\n",
    "base_opt_SCR_simi_nw_full = nw_fea_arrs_dict[\"SCR\"][best_distance_measures[\"SCR NW\"]][\"simi\"][\"full\"]\n",
    "base_opt_LAB_simi_nw_full = nw_fea_arrs_dict[\"LAB\"][best_distance_measures[\"LAB NW\"]][\"simi\"][\"full\"]\n",
    "fix_combined_weights_dict_nw = combine_best_weights_for_test(base_opt_SCR_simi_nw_full, base_opt_LAB_simi_nw_full, A_nw, B_nw, train_idx, test_idx, y_full, num_processors)\n",
    "\n",
    "for k in tqdm(k_sizes_test):\n",
    "    base_AUPRC_nw_KNN, base_AUROC_nw_KNN = KNN(fix_combined_weights_dict_nw, k, y_test)\n",
    "    base_opt_distance_performance_nw_KNN[\"AUPRC\"].append(base_AUPRC_nw_KNN)\n",
    "    base_opt_distance_performance_nw_KNN[\"AUROC\"].append(base_AUROC_nw_KNN)\n",
    "    base_AUPRC_nw_LR, base_AUROC_nw_LR = predict_by_LR(X_train, X_test, fix_combined_weights_dict_nw, k, y_test)\n",
    "    base_opt_distance_performance_nw_LR[\"AUPRC\"].append(base_AUPRC_nw_LR)\n",
    "    base_opt_distance_performance_nw_LR[\"AUROC\"].append(base_AUROC_nw_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 5: Global Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_norm = X_train.copy(deep = True)\n",
    "X_test_norm = X_test.copy(deep = True) \n",
    "X_train_norm.loc[:, :] = min_max_normalization(X_train_norm, axis = 0)\n",
    "X_test_norm.loc[:, :] = min_max_normalization(X_test_norm, axis = 0)\n",
    "X_full_norm = pd.concat([X_train_norm, X_test_norm], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, X_Euc_idx_full, _, _ = compute_similarity(X_full_norm, 'euclidean', train_len, num_processors)\n",
    "Euc_idx_arr_test_glob, Euc_y_test_arr_glob = process_idx_arr_for_test(train_idx, test_idx, X_Euc_idx_full, y_full)\n",
    "glob_Euc_idx_y_dict_test = {\"idx\": Euc_idx_arr_test_glob, \"label\": Euc_y_test_arr_glob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_glob_Euc_performance_KNN = {\"AUPRC\": [], \"AUROC\": []}\n",
    "base_glob_Euc_performance_LR = {\"AUPRC\": [], \"AUROC\": []}\n",
    "for k in tqdm(k_sizes_test):\n",
    "    base_AUPRC_glob_KNN, base_AUROC_glob_KNN = KNN(glob_Euc_idx_y_dict_test, k, y_test)\n",
    "    base_glob_Euc_performance_KNN[\"AUPRC\"].append(base_AUPRC_glob_KNN)\n",
    "    base_glob_Euc_performance_KNN[\"AUROC\"].append(base_AUROC_glob_KNN)\n",
    "    base_AUPRC_glob_LR, base_AUROC_glob_LR = predict_by_LR(X_train, X_test, glob_Euc_idx_y_dict_test, k, y_test)\n",
    "    base_glob_Euc_performance_LR[\"AUPRC\"].append(base_AUPRC_glob_LR)\n",
    "    base_glob_Euc_performance_LR[\"AUROC\"].append(base_AUROC_glob_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance 6: Global Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, X_Cos_idx_full, _, _ = compute_similarity(X_full_norm, 'cosine', train_len, num_processors)\n",
    "Cos_idx_arr_test_glob, Cos_y_test_arr_glob = process_idx_arr_for_test(train_idx, test_idx, X_Cos_idx_full, y_full)\n",
    "glob_Cos_idx_y_dict_test = {\"idx\": Cos_idx_arr_test_glob, \"label\": Cos_y_test_arr_glob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_glob_Cos_performance_KNN = {\"AUPRC\": [], \"AUROC\": []}\n",
    "base_glob_Cos_performance_LR = {\"AUPRC\": [], \"AUROC\": []}\n",
    "for k in tqdm(k_sizes_test):\n",
    "    base_AUPRC_glob_KNN, base_AUROC_glob_KNN = KNN(glob_Cos_idx_y_dict_test, k, y_test)\n",
    "    base_glob_Cos_performance_KNN[\"AUPRC\"].append(base_AUPRC_glob_KNN)\n",
    "    base_glob_Cos_performance_KNN[\"AUROC\"].append(base_AUROC_glob_KNN)\n",
    "    base_AUPRC_glob_LR, base_AUROC_glob_LR = predict_by_LR(X_train, X_test, glob_Cos_idx_y_dict_test, k, y_test)\n",
    "    base_glob_Cos_performance_LR[\"AUPRC\"].append(base_AUPRC_glob_LR)\n",
    "    base_glob_Cos_performance_LR[\"AUROC\"].append(base_AUROC_glob_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Final Performance of Personalized Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.Plotting import plot_final_performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6)) \n",
    "\n",
    "plot_final_performance_metrics(axs[0], k_sizes_test, \"AUPRC\", base_glob_Euc_performance_KNN, base_glob_Cos_performance_KNN, base_fix_distance_performance_nw_KNN, base_opt_distance_performance_nw_KNN, final_model_performance_nw_KNN, final_model_performance_wt_KNN, \"Equal Voting\")\n",
    "plot_final_performance_metrics(axs[1], k_sizes_test, \"AUROC\", base_glob_Euc_performance_KNN, base_glob_Cos_performance_KNN, base_fix_distance_performance_nw_KNN, base_opt_distance_performance_nw_KNN, final_model_performance_nw_KNN, final_model_performance_wt_KNN, \"Equal Voting\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6)) \n",
    "\n",
    "plot_final_performance_metrics(axs[0], k_sizes_test, \"AUPRC\", base_glob_Euc_performance_LR, base_glob_Cos_performance_LR, base_fix_distance_performance_nw_LR, base_opt_distance_performance_nw_LR, final_model_performance_nw_LR, final_model_performance_wt_LR, \"Personalized Lostic Regression\")\n",
    "plot_final_performance_metrics(axs[1], k_sizes_test, \"AUROC\", base_glob_Euc_performance_LR, base_glob_Cos_performance_LR, base_fix_distance_performance_nw_LR, base_opt_distance_performance_nw_LR, final_model_performance_nw_LR, final_model_performance_wt_LR, \"Personalized Lostic Regression\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_personalized_modeling",
   "language": "python",
   "name": "aki_personalized_modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
